{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Segmentation of Brain Abnormalities\n",
    "\n",
    "In this notebook, I want to challenge myself by implementing a Unet model to segment MR images of the brain abnormalities pixel by pixel. The dataset I will be using contains brain MR images together with manual FLAIR abnormality segmentation masks obtained from The Cancer Imaging Archive (TCIA).\n",
    "Images correspond to 110 patients included in The Cancer Genome Atlas (TCGA) lower-grade glioma collection with at least fluid-attenuated inversion recovery (FLAIR) sequence and genomic cluster data available. (Data source: [The Cancer Imagin Archrive](https://wiki.cancerimagingarchive.net/display/Public/TCGA-LGG#6abaca285cee4c9cac59b0bcff944658))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependencies (Required Frameworks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tf.keras.backend.backend()\n",
    "\n",
    "# Imaging frameworks\n",
    "from PIL import Image\n",
    "from skimage.transform import rotate, rescale\n",
    "import skimage.io\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning and Loading the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each patient MR Images and segmentation Masks have been stored in the same directory. Masks have the same name as images plus a \"_mask\" string.\n",
    "\n",
    "First, we remove the unused files and save Images and Masks in 2 different directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The directory of the image dataset\n",
    "Dataset_path = \"~/tmp/TCGA-LGG\" \n",
    "\n",
    "# Access to the directory from Python and Remove unused data\n",
    "files=os.listdir(Dataset_path) \n",
    "folders.remove('data.csv') \n",
    "folders.remove('README.md') \n",
    "\n",
    "# Masks and Images temporary paths\n",
    "temp_image_path, temp_mask_path= [], [] \n",
    "\n",
    "# Reading folders containing Images and Masks for each patient\n",
    "for folder in tqdm(folders): \n",
    "    img_mask_path = os.path.join(Dataset_path, folder)\n",
    "    files = os.listdir(img_mask_path) \n",
    "    \n",
    "# Saving Images and corresponding Masks in separate directories    \n",
    "for file in tqdm(files): \n",
    "    if \"mask.tif\" not in file.split('_'): \n",
    "        temp_image_path.append(os.path.join(Dataset_path, file))\n",
    "    else:\n",
    "        temp_mask_path.append(os.path.join(Dataset_path, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "temp_image_path and temp_mask_path include MR Images and segmentation Masks for all patients.\n",
    "\n",
    "If there are images without masks,the below code will delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, mask_path = [], [] #Masks and images paths\n",
    "img_wo_mask = [] #Images without masks paths\n",
    "\n",
    "# If an Image matches a Mask, parallelly store them in two separate paths \n",
    "for img in tqdm(temp_image_path):\n",
    "    img_mask = img.split(\".\")[0] + \"_mask\" \n",
    "    for mask in temp_mask_path:\n",
    "        if img_mask == mask.split(\".\")[0]:\n",
    "            img_path.append(img), mask_path.append(mask) \n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "# If the len(img_path) != len(mask_path), add image to img_wo_mask           \n",
    "if len(img_path) == len(temp_image_path): \n",
    "    print('\\033[1m' + \"All images have mask!\")    \n",
    "else: \n",
    "    for img in temp_image_path:\n",
    "        if img not in img_path:\n",
    "            img_wo_mask. append(img)\n",
    "            print('\\033[1m' + img + \" does not have mask!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spliting the data into Train, Validate and Test data sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasetdir(directory):\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        print('\\033[1m' + \"Directory \" , directory ,  \" Created \") \n",
    "    except FileExistsError:\n",
    "        print('\\033[1m' + \"Directory \" , directory ,  \" already exists\")\n",
    "\n",
    "train_dir = \"~/tmp/train\"        \n",
    "val_dir = \"~/tmp/validate\"\n",
    "test_dir = \"~/tmp/test\"\n",
    "\n",
    "train_dir = datasetdir(train_dir)\n",
    "val_dir = datasetdir(val_dir)\n",
    "test_dir = datasetdir(test_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data inside the img_path and msk_path into three directories of train_dir, val_dir and test_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(2020)\n",
    "random.shuffle(img_path) #shuffle images\n",
    "random.shuffle(mask_path) #shuffle masks\n",
    "\n",
    "train_split = int(0.7*len(img_path)) #train: 70%\n",
    "val_split = int(0.9 * len(img_path)) #val: 20% & test: 10% \n",
    "\n",
    "def dataset(img_path, img_dir, split1, split2):\n",
    "    images = img_path[split1:split2]\n",
    "    for i in tqdm(images): Image.open(i).save(img_dir + \"\\\\{}\".format(i.split(\"\\\\\")[-1])) \n",
    "    return images\n",
    "\n",
    "train_images = dataset(img_path, train_dir, None, train_split)\n",
    "train_masks = dataset(img_path, train_dir, None, train_split)\n",
    "val_images = dataset(img_path, val_dir, train_split, val_split)\n",
    "val_masks = dataset(img_path, val_dir, train_split, val_split)\n",
    "test_images = dataset(img_path, test_dir, val_split, None)\n",
    "test_masks = dataset(img_path, test_dir, val_split, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Augmentation:\n",
    "\n",
    "The performance of deep learning neural networks often improves with the amount of data available.\n",
    "\n",
    "Data augmentation is a technique to artificially create new training data from existing training data. This is done by applying domain-specific techniques to examples from the training data that create new and different training examples.\n",
    "\n",
    "Image data augmentation is perhaps the most well-known type of data augmentation and involves creating transformed versions of images in the training dataset that belong to the same class as the original image.\n",
    "\n",
    "Transforms include a range of operations from the field of image manipulation, such as shifts, flips, zooms, and much more.\n",
    "\n",
    "In this section, we will use two techniques including rotating and scaling images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_img(img, img_mask):\n",
    "    \"\"\" Rotating images randomly from 5.0 to 15.0 units \"\"\"\n",
    "    \n",
    "    angle = np.random.uniform(5.0, 15.0) * np.random.choice([-1.0, 1.0], 1)[0]\n",
    "\n",
    "    img = rotate(img, angle, resize=False, order=3, preserve_range=True)\n",
    "    img_mask = rotate(img_mask, angle, resize=False, order=0, preserve_range=True)\n",
    "\n",
    "    return img, img_mask\n",
    "\n",
    "def scale_img(img, img_mask, img_height = 256, img_width = 256): \n",
    "    \"\"\" Scaling images randomly between 0.04 and 0.08 units\"\"\"\n",
    "    \n",
    "    scale = 1.0 + np.random.uniform(0.04, 0.08) * np.random.choice([-1.0, 1.0], 1)[0]\n",
    "\n",
    "    img = rescale(img, scale, order=3, preserve_range=True)\n",
    "    img_mask = rescale(img_mask, scale, order=0, preserve_range=True)\n",
    "    if scale > 1:\n",
    "        img = center_crop(img, img_height, img_width)\n",
    "        img_mask = center_crop(img_mask, img_height, img_width)\n",
    "    else:\n",
    "        img = zeros_pad(img, img_height)\n",
    "        img_mask = zeros_pad(img_mask, img_height)\n",
    "\n",
    "    return img, img_mask\n",
    "\n",
    "def center_crop(img, cropx, cropy):\n",
    "    \"\"\" Cropping the center of images \"\"\"\n",
    "    \n",
    "    x = img.shape[1] // 2 - (cropx // 2)\n",
    "    y = img.shape[0] // 2 - (cropy // 2)\n",
    "    return img[y : y + cropy, x : x + cropx]\n",
    "\n",
    "def zeros_pad(img, size): \n",
    "    \"\"\" Zero-padding images \"\"\"\n",
    "    \n",
    "    pad_before = int(round(((size - img.shape[0]) / 2.0)))\n",
    "    pad_after = size - img.shape[0] - pad_before\n",
    "    if len(img.shape) > 2:\n",
    "        return np.pad(img, ((pad_before, pad_after), (pad_before, pad_after), (0, 0)), mode=\"constant\")\n",
    "    return np.pad(img, (pad_before, pad_after), mode=\"constant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug(images, masks):\n",
    "\n",
    "    images_augmentation = []\n",
    "    masks_augmentation = []\n",
    "    \n",
    "    # Augment images if the corresponding segmentation Mask is not None\n",
    "    for i in tqdm(range(len(images))): \n",
    "        if np.max(masks[i]) < 1:\n",
    "            continue\n",
    "            \n",
    "        #rotating\n",
    "        image_rotate, mask_rotate = rotate_img(images[i], masks[i]) \n",
    "        images_augmentation.append(image_rotate)\n",
    "        masks_augmentation.append(mask_rotate)\n",
    "        \n",
    "        #scaling\n",
    "        image_scale, mask_scale = scale_img(images[i], masks[i]) \n",
    "        images_augmentation.append(image_scale)\n",
    "        masks_augmentation.append(mask_scale)\n",
    "            \n",
    "        # Duplicate the data if the segmentation mask is not None\n",
    "        for _ in range(2): \n",
    "            images_augmentation.append(images[i])\n",
    "            masks_augmentation.append(masks[i])\n",
    "\n",
    "    images_augmentation = np.array(images_augmentation)\n",
    "    masks_augmentation = np.array(masks_augmentation)\n",
    "    \n",
    "    return np.vstack((images, images_augmentation)), np.vstack((masks, masks_augmentation)) #add images and augmented images together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model's input data:\n",
    "\n",
    "As the final step, we read the data as matrices of numbers ready to feed into the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data(path, img_height = 256, img_width = 256, channels = 3, augmentation = True):\n",
    "    \n",
    "    images_list = os.listdir(path)\n",
    "    \n",
    "    total_count = int(len(images_list) / 2)\n",
    "    \n",
    "    images = np.ndarray((total_count, img_height, img_width), dtype=np.uint8) \n",
    "    masks = np.ndarray((total_count, img_height, img_width), dtype=np.uint8) \n",
    "    names = np.chararray(total_count, itemsize=64)\n",
    "\n",
    "    i = 0\n",
    "    for image_name in tqdm(images_list):\n",
    "        if \"mask\" in image_name:\n",
    "            continue\n",
    "\n",
    "        names = image_name.split(\".\")[0]\n",
    "        slice_number = int(names.split(\"_\")[-1])\n",
    "        patient_id = \"_\".join(names.split(\"_\")[:-1])\n",
    "\n",
    "        img = skimage.io.imread(os.path.join(path, image_name), as_gray=True) #read the image\n",
    "        images[i] = img\n",
    "        \n",
    "        image_mask_name = image_name.split(\".\")[0] + \"_mask.tif\"\n",
    "        img_mask = skimage.io.imread(os.path.join(path, image_mask_name), as_gray=True) #read the mask\n",
    "        img_mask = cv2.resize(img_mask, (128, 128), interpolation=cv2.INTER_NEAREST)\n",
    "        img_mask = np.array([img_mask])\n",
    "        masks[i] = img_mask\n",
    "        \n",
    "        i +=1\n",
    "        \n",
    "    images = images[..., np.newaxis]\n",
    "    images = images.astype(\"float32\")\n",
    "    mean = np.mean(images)\n",
    "    std = np.std(images)\n",
    "    images -= mean\n",
    "    images /= std\n",
    "    \n",
    "    masks = masks[..., np.newaxis]\n",
    "    masks = masks.astype(\"float32\")\n",
    "    masks /= 255.\n",
    "    \n",
    "    if augmentation == True: \n",
    "        images, masks = aug(images, masks)\n",
    "\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unet Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [**UNET**](https://arxiv.org/abs/1505.04597) was developed by Olaf Ronneberger et al. for Bio Medical Image Segmentation. The architecture contains two paths. First path is the contraction path (also called as the encoder) which is used to capture the context in the image. The encoder is just a traditional stack of convolutional and max pooling layers. The second path is the symmetric expanding path (also called as the decoder) which is used to enable precise localization using transposed convolutions. Thus it is an end-to-end fully convolutional network (FCN), i.e. it only contains Convolutional layers and does not contain any Dense layer because of which it can accept image of any size.\n",
    "\n",
    "In the original paper, the **UNET** is described as follows:\n",
    "\n",
    "![U-net](https://miro.medium.com/max/680/1*TXfEPqTbFBPCbXYh2bstlA.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(img_height = 256, img_width = 256, channels = 3):\n",
    "    \n",
    "    \"\"\"UNET structure implemented using TensorFlow\"\"\"\n",
    "    \n",
    "    inputs = Input((img_height, img_width, channels)) #Input image information\n",
    "    \n",
    "    #Filters = [32, 64, 128, 256, 512]\n",
    "    \n",
    "    conv1 = Conv2D(32, (3, 3), padding='same')(inputs) #3 by 3 convolution with same padding\n",
    "    conv1 = Activation('relu')(conv1) #relu activation\n",
    "    conv1 = Conv2D(32, (3, 3), padding='same')(conv1) #3 by 3 convolution with same padding\n",
    "    conv1 = Activation('relu')(conv1) #relu activation\n",
    "    Max_pool1 = MaxPooling2D(pool_size=(2, 2))(conv1) #Maxpooling filter \n",
    "    \n",
    "    conv2 = Conv2D(64, (3, 3), padding='same')(Max_pool1) #3 by 3 convolution with same padding\n",
    "    conv2 = Activation('relu')(conv2) #relu activation\n",
    "    conv2 = Conv2D(64, (3, 3), padding='same')(conv2) #3 by 3 convolution with same padding\n",
    "    conv2 = Activation('relu')(conv2) #relu activation\n",
    "    Max_pool2 = MaxPooling2D(pool_size=(2, 2))(conv2) #Maxpooling filter\n",
    "    \n",
    "    conv3 = Conv2D(128, (3, 3), padding='same')(Max_pool2) #3 by 3 convolution with same padding\n",
    "    conv3 = Activation('relu')(conv3) #relu activation\n",
    "    conv3 = Conv2D(128, (3, 3), padding='same')(conv3) #3 by 3 convolution with same padding\n",
    "    conv3 = Activation('relu')(conv3) #relu activation\n",
    "    Max_pool3 = MaxPooling2D(pool_size=(2, 2))(conv3) #Maxpooling filter\n",
    "    \n",
    "    conv4 = Conv2D(256, (3, 3), padding='same')(Max_pool3) #3 by 3 convolution with same padding\n",
    "    conv4 = Activation('relu')(conv4) #relu activation\n",
    "    conv4 = Conv2D(256, (3, 3), padding='same')(conv4) #3 by 3 convolution with same padding\n",
    "    conv4 = Activation('relu')(conv4) #relu activation\n",
    "    Max_pool4 = MaxPooling2D(pool_size=(2, 2))(conv4) #Maxpooling filter\n",
    "    \n",
    "    conv5 = Conv2D(512, (3, 3), padding='same')(Max_pool4) #3 by 3 convolution with same padding\n",
    "    conv5 = Activation('relu')(conv5) #relu activation\n",
    "    conv5 = Conv2D(512, (3, 3), padding='same')(conv5) #3 by 3 convolution with same padding\n",
    "    conv5 = Activation('relu')(conv5) #relu activation\n",
    "    \n",
    "    upconv1 = Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same')(conv5) #2 by 2 upconvolution with (2, 2) strides\n",
    "    concat1 = concatenate([upconv1, conv4], axis=3)\n",
    "    \n",
    "    conv6 = Conv2D(256, (3, 3), padding='same')(concat1) #3 by 3 convolution with same padding\n",
    "    conv6 = Activation('relu')(conv6) #relu activation\n",
    "    conv6 = Conv2D(256, (3, 3), padding='same')(conv6) #3 by 3 convolution with same padding\n",
    "    conv6 = Activation('relu')(conv6) #relu activation\n",
    "    \n",
    "    upconv2 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(conv6) #2 by 2 upconvolution with (2, 2) strides\n",
    "    concat2 = concatenate([upconv2, conv3], axis=3) #Adding the encoder to decoder output\n",
    "    \n",
    "    conv7 = Conv2D(128, (3, 3), padding='same')(concat2) #3 by 3 convolution with same padding\n",
    "    conv7 = Activation('relu')(conv7) #relu activation\n",
    "    conv7 = Conv2D(128, (3, 3), padding='same')(conv7) #3 by 3 convolution with same padding\n",
    "    conv7 = Activation('relu')(conv7) #relu activation\n",
    "    \n",
    "    upconv3 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(conv7) #2 by 2 upconvolution with (2, 2) strides\n",
    "    concat3 = concatenate([upconv3, conv2], axis=3) #Adding the encoder to decoder output\n",
    "    \n",
    "    conv8 = Conv2D(64, (3, 3), padding='same')(concat3) #3 by 3 convolution with same padding\n",
    "    conv8 = Activation('relu')(conv8) #relu activation\n",
    "    conv8 = Conv2D(64, (3, 3), padding='same')(conv8) #3 by 3 convolution with same padding\n",
    "    conv8 = Activation('relu')(conv8) #relu activation\n",
    "    \n",
    "    upconv4 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(conv8) #2 by 2 upconvolution with (2, 2) strides\n",
    "    concat4 = concatenate([upconv4, conv1], axis=3) #Adding the encoder to decoder output\n",
    "    \n",
    "    conv9 = Conv2D(32, (3, 3), padding='same')(concat4) #3 by 3 convolution with same padding\n",
    "    conv9 = Activation('relu')(conv9) #relu activation\n",
    "    conv9 = Conv2D(32, (3, 3), padding='same')(conv9) #3 by 3 convolution with same padding\n",
    "    conv9 = Activation('relu')(conv9) #relu activation\n",
    "    \n",
    "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(conv9) #1 by 1 convolution with sigmoid activation\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[outputs]) #The output model\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss Function:\n",
    "\n",
    "The Dice similarity coefficient, also known as the Sørensen–Dice index or simply Dice coefficient, is a statistical tool which measures the similarity between two sets of data. This index has become arguably the most broadly used tool in the validation of image segmentation algorithms created with AI, but it is a much more general concept which can be applied sets of data for a variety of applications including NLP.\n",
    "\n",
    "The equation for this concept is:\n",
    "2 * |X ∩ Y| / (|X| + |Y|)\n",
    "\n",
    "where X and Y are two sets\n",
    "a set with vertical bars either side refers to the cardinality of the set, i.e. the number of elements in that set, e.g. |X| means the number of elements in set X\n",
    "∩ is used to represent the intersection of two sets, and means the elements that are common to both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_coef(true, pred):\n",
    "    flatten_true = tf.keras.backend.flatten(true)\n",
    "    flatten_pred = tf.keras.backend.flatten(pred)\n",
    "    intersection = tf.keras.backend.sum(flatten_true * flatten_pred)\n",
    "    \n",
    "    return (2. * intersection + 1.0) / (tf.keras.backend.sum(flatten_true) + \n",
    "                                        tf.keras.backend.sum(flatten_pred) + 1.0) #Smooth = 1.0\n",
    "\n",
    "def dice_coef_loss(true, pred):\n",
    "    return 1.0 - dice_coef(true, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = network(img_height = 256, img_width = 256, channels = 1)\n",
    "optimizer = tf.keras.optimizers.Adam(lr=1e-5) #Adam optimizer with 1e-5 learning rate\n",
    "model.compile(optimizer=optimizer, loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"~/tmp/training_weights\"\n",
    "train_path = train_dir\n",
    "val_path = val_dir \n",
    "batch_size = 16\n",
    "epochs = 30\n",
    "\n",
    "def train(train_images, train_masks, val_images, val_masks, weights_path):\n",
    "    \"\"\" Train the network using the train and val data \"\"\"\n",
    "    \n",
    "    model.fit(\n",
    "        train_images,\n",
    "        train_masks,\n",
    "        validation_data=(val_images, val_masks),\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        shuffle=True)\n",
    "\n",
    "    #Save training weights\n",
    "    if not os.path.exists(weights_path):\n",
    "        os.mkdir(weights_path)\n",
    "    model.save_weights(os.path.join(weights_path, \"weights_{}.h5\".format(epochs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_masks = data(train_path, img_height = 256, img_width = 256, channels = 3, augmentation = False)\n",
    "val_images, val_masks = data(val_path, img_height = 256, img_width = 256, channels = 3, augmentation = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(train_images, train_masks, val_images, val_masks, weights_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting and plotting unseen data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_weight = \"~/tmp/weights_30.h5\"\n",
    "\n",
    "def predict(test_path, model_weight):\n",
    "    \"\"\" Predict Masks for the input Images \"\"\"\n",
    "    \n",
    "    model = network(img_height = 256, img_width = 256, channels = 3)    \n",
    "    model.load_weights(model_weight)\n",
    "\n",
    "    # make predictions\n",
    "    pred_masks = model.predict(test_images, verbose=1)\n",
    "\n",
    "    \n",
    "\n",
    "    return test_images, test_masks, pred_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images, test_masks, pred_masks = predict(test_dir, model_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def myshow(image, squeeze, rgb2gray):\n",
    "    \"\"\" myshow plot the MR Image, Ground Truth Mask and Predicted Mask \"\"\"\n",
    "    \n",
    "    if rgb2gray == True:\n",
    "        img = skimage.color.rgb2gray(image) #Rgb2Gray test images\n",
    "        \n",
    "    if squeeze == True: \n",
    "        img = np.squeeze(image) #Squeeze Mask images\n",
    "\n",
    "    return img\n",
    "\n",
    "print('\\033[1m' + \"\\n    Original Image            Ground Truth Mask           Predicted Mask\")\n",
    "\n",
    "for i in range(len(test_images)): \n",
    "        if np.max(test_masks[i]) < 1:\n",
    "            continue\n",
    "        \n",
    "        #Plotting images\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        fig.subplots_adjust(hspace=0.1, wspace=0.1)\n",
    "        ax = fig.add_subplot(1,3,1)\n",
    "        ax.imshow(myshow(test_images[i], squeeze=False, rgb2gray=True), cmap=\"gray\")\n",
    "        ax.axis(False)\n",
    "        ax = fig.add_subplot(1,3,2)\n",
    "        ax.imshow(myshow(test_masks[i], squeeze=True, rgb2gray=False), cmap=\"gray\")\n",
    "        ax.axis(False)\n",
    "        ax = fig.add_subplot(1,3,3)\n",
    "        ax.imshow(myshow(pred_masks[i], squeeze=True, rgb2gray=False), cmap=\"gray\")\n",
    "        ax.axis(False)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
